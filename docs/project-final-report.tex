\documentclass[letterpaper,twocolumn,10pt]{article}
\usepackage{usenix,epsfig,endnotes,enumerate,graphicx,multicol}
\begin{document}

%don't want date printed
\date{}

%make title bold and 14 pt font (Latex default is non-bold, 16 pt)
\title{\Large \bf DLisp: Automatically distributed computation}

\newcommand{\tuftsauthor}[1]{{\rm #1}\\
  Tufts University}

\author{
  \tuftsauthor{Maxwell Bernstein}
  \and
  \tuftsauthor{Matthew Yaspan}
}

\maketitle

\subsection*{Abstract}
Many of today's programs are written sequentially, and do not take advantage of
the computer's available resources. This is in a large part due to the
difficulty of using any given available threading or parallelization API.
Moreover, these programs often fail to take advantage of the network
to distribute work not just among processes to take advantage of the
scheduler, but use available computing power over the network.

We solve this problem by automatically parallelizing or distributing
computation across cores or even across a datacenter, then analyze the
performance of our distribution algorithms across several modes. We
created a toy programming language based on lisp and built the underlying
parser, basis, and evaluator in Erlang, which also handles our network
protocols and distribution algorithms.

\section{Design}

\subsection{Language}

We began with a Lisp-like language with support for most forms: \verb|fixnum|,
\verb|boolean|, \verb|symbol|, \verb|lambda|, \verb|funcall|, \verb|define|,
\verb|val|, \verb|quote|, \verb|if|, \verb|let|, \verb|let*|, \verb|eval|,
\verb|apply|, built-in functions, and closures. Then we decided that users
would be less comfortable with a Lisp than a programming language with a syntax
that mirrors existing programming languages like SML and OCaml, and changed the
syntax. Mergesort, for example, begin as:

\begin{verbatim}
(define mergesort (xs)
  (if (or (null? xs) (null? (cdr xs)))
    xs
    (let* ((size    (length xs))
           (half    (/ size 2))
           (fsthalf (take half xs))
           (sndhalf (drop half xs)))
      (merge (mergesort fsthalf)
             (mergesort sndhalf)))))
\end{verbatim}

\newpage

and, after the syntax transformation, ended as:

\begin{verbatim}
fun mergesort(xs) =
  if null?(xs) or null?(cdr(xs))
  then xs
  else let* val size = length(xs),
            val half = size/2,
            val fsthalf = take(half, xs),
            val sndhalf = drop(half, xs)
       in merge(mergesort(fsthalf), mergesort(sndhalf))
       end;;
\end{verbatim}

Both programs map to the same abstract syntax tree, which means that we even
allow mixing and matching of styles in the same program, as in:

\begin{verbatim}
fun mergesort(xs) =
  if [or [null? xs] [null? [cdr xs]]]
  then xs
  else let* val size = length(xs),
            val half = size/2,
            val fsthalf = take(half, xs),
            val sndhalf = drop(half, xs)
       in [merge mergesort(fsthalf) mergesort(sndhalf)]
       end;;
\end{verbatim}

One language property that \textit{did not} change in the transition was
mutation; DLisp forces variable immutability. In forms that introduce new
environments, such as \verb|let| and \verb|lambda|, shadowing is allowed -- but
never mutation. This, as it turns out, is key when attempting to parallelize
computation.

For this reason, \verb|map|, \verb|pmap| (parallel map), and \verb|dmap|
(distributed map) are all built-in special forms.

\newpage

\subsection{Network}

We use several terms across the span of this writeup:

\begin{itemize}
    \item \textit{Master} --- Main controller computer from which the program
        is run and distributed. Communicates with several Machines and Workers.
    \item \textit{Machine} --- Logical or physical computer, which contains
        many Agents and Workers.
    \item \textit{WorkPacket} --- A serializable tuple of the form \verb|{Exp|
        $x$ \verb|Env}| that is sent to Workers.
    \item \textit{Worker} --- Process whose sole purpose is to receive
        WorkPackets, evaluate them, and send the results back to the Master.
    \item \textit{SlowWorker} --- A Worker that has been artificially slowed
        down by a constant factor.
    \item \textit{Agent} --- Process whose sole purpose is to manage a work
        queue.
    \item \textit{StealingAgent} --- Agent that occasionally steals from other
        Agents when its worker is moving quickly.
    \item \textit{RoundRobinMode} --- Distribution mode that uses a circular
        queue to hand out work to Workers in order.
    \item \textit{ByMachineMode} --- Distribution mode that uses per-Machine
        statistics to determine which Worker should receive a given WorkPacket.
        Currently, this has two sub-modes:
        \begin{itemize}
            \item \textit{Timed} --- Hand out work to whoever can respond the
                fastest to a HealthCheck.
            \item \textit{Memory} --- Hand out work to whoever has the most
                computational power (currently measured by memory pressure)
                currently available.
        \end{itemize}
\end{itemize}

We use normal (non-stealing) Agents to begin with, then proceed to demonstrate
the utility and speed gains by using StealingAgents. Additionally, we introduce
some SlowWorkers into the Worker pool to demonstrate that work stealing is an
effective means of combating heterogeneous computational power.

Additionally, we demonstrate the results of different distribution modes
(enumerated above) and their affects on end-to-end computation speed.

\subsection{Startup Procedure}

A Master node is started, and runs on node \verb|M|. Independently, anywhere
from 0 to N>0 Machines are started up on the same network, with knowledge of
the Master. In this case, we use Erlang nodenames, such as the atom
\verb|master@some.ip.address.here|, to identify the Master.

\section{Distribution Methods}

The goal of this project was to allow for a client with basic coding abilities
to vastly improve the performance of their program which may take up a significant
amount of memory or processing power by distributing it in a more effective way,
either  by using Erlang's ability to spawn threads and collate responses to take
advantage of the local scheduler, or by distributing the work across multiple machines
over network.

\subsection{Local Parallelization}

The function 'pmap' in dlisp takes in a function, which can be anonymous, and 
a list, as arguments. The parser decomposes these inputs into a list of WorkPackets
consisting an expression and the environment in which the expression is to be carried out.
The expression consists of an operation and a member of the list on which it is to be
evaluated. For each member of the list, an Erlang process is spawned in which our eval
module is called on the WorkPacket and then the evaluated result is sent back to the Master
Erlang process. An assemble function collates all of the results and returns the mapped list.

The advantage of this is that it allows for more optimal scheduling of processes that are not
dependent on one another. Because this is not a Reduce operation, there's no data dependency
to resolve, and evaluating sequentially wastes scheduling time for no discernible benefit.

\subsection{Distributed Parallelization}

The function 'dmap' in dlisp works semantically just like pmap. However, under the hood, 
a significant amount is different. There are three possible methods of distributed map:
Round Robin, Low Latency, and By Memory.

\subsubsection{Round Robin}

In the round robin scheme, all machines that are to contain Worker processes are initialized
with a number of processes calculate based on the amount of memory and/or number cores on the
respective machines. Each machine sends their list of Worker processes identifiers to master, 
and the Master concatenates the lists into a queue and randomizes the order of the workers.
When dmap is  invoked, The 'dmap' call is decomposed in the same way as 'pmap' into WorkPackets
for each item in the list. In the case of dmap, however, the WorkPacket is sent to processes
selected from the aforementioned queue. A process is popped off the queue, sent a WorkPacket,
and requeued at the back, in a round-robin scheme. After all of the work is sent, a list of IDs
for each individual job is returned so that when Workers send results back, they can be collated
and assembled into a mapped list by master who sits in receiving mode until all the packets have
been recovered. 


\subsection{Low Latency}

In the low latency scheme, initialization occurs by machine. When machines register with master,
It is appended to a list of Machine representations, each element containing the machine Pid, 
a queue of Processes local to the Machine. When 'dmap' is called, for each iteration through the
list given as an argument, a message is sent to all of the worker Machines, and the first one to
respond is given the WorkPacket. The process is then repeated until work is entirely allocated.
The goal of this algorithm is to account for disparities in latency between machines. Although this
was not an issue we experienced, it is plausible that a large organization with multiple data centers
could see this issue if the pool of machines contained nodes with a significant enough distance from
each other or nodes that are simply slower. Collation works the same as in Round Robin, where
the Master waits for each packet to return before in order returning the collated results.

\subsection{By Memory}

The By Memory scheme is similar to low latency, except in this case available memory is prioritized.
Much like in the low latency scheme, initialization occurs by 

\end{document}
